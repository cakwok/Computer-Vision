{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Wing Man Casca, Kwok<br>\nSep 18 2022<br>\nCS 7180 Advanced Perception\n\nRemark: Most of the coding here is referenced from https://www.kaggle.com/code/balraj98/single-image-super-resolution-gan-srgan-pytorch/notebook by BALRAJ ASHWATH.  Having said that, inspired by the usage of super resolution, I have created new codes by scaling a realistically sized image (850 x 850) to 3x to 2500 x 2500, to observe behaviour of SRGAN with image quality we use daily life (not intentionally taken or retorched for training and testing).","metadata":{}},{"cell_type":"code","source":"#Import Libraries\nimport numpy as np\nimport pandas as pd\nimport os, math, sys\nimport glob, itertools\nimport argparse, random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.models import vgg19\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image, make_grid\n\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-09-19T07:52:58.237356Z","iopub.execute_input":"2022-09-19T07:52:58.237788Z","iopub.status.idle":"2022-09-19T07:53:00.486194Z","shell.execute_reply.started":"2022-09-19T07:52:58.237740Z","shell.execute_reply":"2022-09-19T07:53:00.485322Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Define hyperparameters\nload_pretrained_models = True\nn_epochs = 1\ndataset_path = \"../input/celeba-dataset/img_align_celeba/img_align_celeba\"\nbatch_size = 16\nlr = 0.0001          # adam: learning rate\nb1 = 0.5             # adam: decay of first order momentum of gradient\nb2 = 0.999           # adam: decay of second order momentum of gradient\nn_cpu = 8            # number of cpu threads to use during batch generation\nhr_height = 256      # high res. image height\nhr_width = 256       # high res. image width\nchannels = 3\n\nos.makedirs(\"images\", exist_ok=True)\nos.makedirs(\"saved_models\", exist_ok=True)\n\ncuda = torch.cuda.is_available()\nhr_shape = (hr_height, hr_width)\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T07:53:00.488337Z","iopub.execute_input":"2022-09-19T07:53:00.488818Z","iopub.status.idle":"2022-09-19T07:53:00.556739Z","shell.execute_reply.started":"2022-09-19T07:53:00.488779Z","shell.execute_reply":"2022-09-19T07:53:00.555831Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Define Dataset and transform images\n\n#Normalization parameters for pre-trained PyTorch models\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\nclass ImageDataset(Dataset):\n    def __init__(self, files, hr_shape):\n        hr_height, hr_width = hr_shape\n        # Transforms for low resolution images and high resolution images\n        self.lr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.hr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.files = files\n    \n    def __getitem__(self, index):\n        img = Image.open(self.files[index % len(self.files)])\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n\n        return {\"lr\": img_lr, \"hr\": img_hr}\n\n    def __len__(self):\n        return len(self.files)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T07:53:00.558912Z","iopub.execute_input":"2022-09-19T07:53:00.559429Z","iopub.status.idle":"2022-09-19T07:53:00.569940Z","shell.execute_reply.started":"2022-09-19T07:53:00.559391Z","shell.execute_reply":"2022-09-19T07:53:00.569060Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Define Dataloaders\ntrain_paths, test_paths = train_test_split(sorted(glob.glob(dataset_path + \"/*.*\")), test_size=0.02, random_state=42)\ntrain_dataloader = DataLoader(ImageDataset(train_paths, hr_shape=hr_shape), batch_size=batch_size, shuffle=True, num_workers=n_cpu)\ntest_dataloader = DataLoader(ImageDataset(test_paths, hr_shape=hr_shape), batch_size=int(batch_size*0.75), shuffle=True, num_workers=n_cpu)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T07:53:00.571277Z","iopub.execute_input":"2022-09-19T07:53:00.571679Z","iopub.status.idle":"2022-09-19T07:53:01.882456Z","shell.execute_reply.started":"2022-09-19T07:53:00.571642Z","shell.execute_reply":"2022-09-19T07:53:01.881512Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Define Neural Networks\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n\n    def forward(self, img):\n        return self.feature_extractor(img)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n            nn.PReLU(),\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n        )\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n\n\nclass GeneratorResNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n        super(GeneratorResNet, self).__init__()\n\n        # First layer\n        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n\n        # Residual blocks\n        res_blocks = []\n        for _ in range(n_residual_blocks):\n            res_blocks.append(ResidualBlock(64))\n        self.res_blocks = nn.Sequential(*res_blocks)\n\n        # Second conv layer post residual blocks\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n\n        # Upsampling layers\n        upsampling = []\n        for out_features in range(2):\n            upsampling += [\n                # nn.Upsample(scale_factor=2),\n                nn.Conv2d(64, 256, 3, 1, 1),\n                nn.BatchNorm2d(256),\n                nn.PixelShuffle(upscale_factor=2),\n                nn.PReLU(),\n            ]\n        self.upsampling = nn.Sequential(*upsampling)\n\n        # Final output layer\n        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n\n    def forward(self, x):\n        out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T07:53:01.885768Z","iopub.execute_input":"2022-09-19T07:53:01.886063Z","iopub.status.idle":"2022-09-19T07:53:01.916619Z","shell.execute_reply.started":"2022-09-19T07:53:01.886035Z","shell.execute_reply":"2022-09-19T07:53:01.915700Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n# Initialize generator and discriminator\ngenerator = GeneratorResNet()\ndiscriminator = Discriminator(input_shape=(channels, *hr_shape))\nfeature_extractor = FeatureExtractor()\n\n# Set feature extractor to inference mode\nfeature_extractor.eval()\n\n# Losses\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_content = torch.nn.L1Loss()\n\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    feature_extractor = feature_extractor.cuda()\n    criterion_GAN = criterion_GAN.cuda()\n    criterion_content = criterion_content.cuda()\n\n# Load pretrained models\nif load_pretrained_models:\n    generator.load_state_dict(torch.load(\"../input/generatorpth/generator.pth\"))\n    discriminator.load_state_dict(torch.load(\"../input/discriminatorpth/discriminator.pth\"))\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.Tensor","metadata":{"execution":{"iopub.status.busy":"2022-09-19T07:53:01.918263Z","iopub.execute_input":"2022-09-19T07:53:01.918828Z","iopub.status.idle":"2022-09-19T07:53:46.393685Z","shell.execute_reply.started":"2022-09-19T07:53:01.918787Z","shell.execute_reply":"2022-09-19T07:53:46.392732Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8f7d3d748344c2d99a169bc51068ab4"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#To train and test the model\ntrain_gen_losses, train_disc_losses, train_counter = [], [], []\ntest_gen_losses, test_disc_losses = [], []\ntest_counter = [idx*len(train_dataloader.dataset) for idx in range(1, n_epochs+1)]\n\nfor epoch in range(n_epochs):\n\n    ### Training\n    gen_loss, disc_loss = 0, 0\n    tqdm_bar = tqdm(train_dataloader, desc=f'Training Epoch {epoch} ', total=int(len(train_dataloader)))\n    \n    for batch_idx, imgs in enumerate(tqdm_bar):\n        generator.train(); discriminator.train()\n         \n        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n        \n        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        \n        ### Train Generator\n        optimizer_G.zero_grad()\n        # Generate a high resolution image from low resolution input\n        gen_hr = generator(imgs_lr)\n        # Adversarial loss\n        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n        # Content loss\n        gen_features = feature_extractor(gen_hr)\n        real_features = feature_extractor(imgs_hr)\n        loss_content = criterion_content(gen_features, real_features.detach())\n        # Total loss\n        loss_G = loss_content + 1e-3 * loss_GAN\n        loss_G.backward()\n        optimizer_G.step()\n\n        ### Train Discriminator\n        optimizer_D.zero_grad()\n        # Loss of real and fake images\n        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n        # Total loss\n        loss_D = (loss_real + loss_fake) / 2\n        loss_D.backward()\n        optimizer_D.step()\n\n        gen_loss += loss_G.item()\n        train_gen_losses.append(loss_G.item())\n        disc_loss += loss_D.item()\n        train_disc_losses.append(loss_D.item())\n        #train_counter.append(batch_idx*batch_size + imgs_lr.size(0) + epoch*len(train_dataloader.dataset))\n        tqdm_bar.set_postfix(gen_loss=gen_loss/(batch_idx+1), disc_loss=disc_loss/(batch_idx+1))\n\n    # Testing\n    gen_loss, disc_loss = 0, 0\n    tqdm_bar = tqdm(test_dataloader, desc=f'Testing Epoch {epoch} ', total=int(len(test_dataloader)))\n    for batch_idx, imgs in enumerate(tqdm_bar):\n        \n        generator.eval(); discriminator.eval()\n        \n        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n       \n        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        \n        ### Eval Generator\n        # Generate a high resolution image from low resolution input\n        gen_hr = generator(imgs_lr)\n        # Adversarial loss\n        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n        # Content loss\n        gen_features = feature_extractor(gen_hr)\n        real_features = feature_extractor(imgs_hr)\n        loss_content = criterion_content(gen_features, real_features.detach())\n        # Total loss\n        loss_G = loss_content + 1e-3 * loss_GAN\n\n        ### Eval Discriminator\n        # Loss of real and fake images\n        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n        # Total loss\n        loss_D = (loss_real + loss_fake) / 2\n\n        gen_loss += loss_G.item()\n        disc_loss += loss_D.item()\n        tqdm_bar.set_postfix(gen_loss=gen_loss/(batch_idx+1), disc_loss=disc_loss/(batch_idx+1))\n        \n        # Saving generated images to visualize result\n        if random.uniform(0,1)<0.1:\n            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n            imgs_hr = make_grid(imgs_hr, nrow=1, normalize=True)\n            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n            img_grid = torch.cat((imgs_hr, imgs_lr, gen_hr), -1)\n            save_image(img_grid, f\"images/{batch_idx}.png\", normalize=False)\n\n    plt.imshow(cv2.cvtColor(cv2.imread(\"images/{batch_idx}.png\"), cv2.COLOR_BGR2RGB))\n    \n    test_gen_losses.append(gen_loss/len(test_dataloader))\n    test_disc_losses.append(disc_loss/len(test_dataloader))\n    \n'''\n    # Save model checkpoints\n    if np.argmin(test_gen_losses) == len(test_gen_losses)-1:\n        torch.save(generator.state_dict(), \"saved_models/generator.pth\")\n        torch.save(discriminator.state_dict(), \"saved_models/discriminator.pth\")\n'''","metadata":{"execution":{"iopub.status.busy":"2022-09-19T07:53:46.397495Z","iopub.execute_input":"2022-09-19T07:53:46.397764Z","iopub.status.idle":"2022-09-19T07:54:04.439756Z","shell.execute_reply.started":"2022-09-19T07:53:46.397736Z","shell.execute_reply":"2022-09-19T07:54:04.438307Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 0 ', max=12410.0, style=ProgressStyle(desc…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa02b2fe85c4502a1f00bcf3a581e3b"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-443e8517f388>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_content\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#To upscale a realistically sized image to 3x\nhr_height = 2500      # high res. image height\nhr_width = 2500       # high res. image width\nhr_shape = (hr_height, hr_width)\n\nrealistic_img_paths = sorted(glob.glob(\"../input/casca-img2/*.*\"))   \nrealistic_img_dataloader = DataLoader(ImageDataset(realistic_img_paths, hr_shape=hr_shape), batch_size=1, shuffle=True, num_workers=n_cpu)\n\nexamples = enumerate(realistic_img_dataloader)\nbatch_idx, imgs = next(examples)\n\ngenerator.eval(); discriminator.eval()\n\nimgs_lr = Variable(imgs[\"lr\"].type(Tensor))\nimgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n  \ngen_hr = generator(imgs_lr)\n\nimgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\nsave_image(imgs_hr, f\"images/imgs_hr.png\", normalize=True)\nsave_image(gen_hr, f\"images/gen_hr.png\", normalize=True)\nsave_image(imgs_lr, f\"images/imgs_lr.png\", normalize=True)\n\nimgs_hr = make_grid(imgs_hr, nrow=1, normalize=True)\ngen_hr = make_grid(gen_hr, nrow=1, normalize=True)\nimgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\nimg_grid = torch.cat((imgs_hr, imgs_lr, gen_hr), -1)\nsave_image(img_grid, f\"images/img_grid.png\", normalize=True)\n\nplt.imshow(cv2.cvtColor(cv2.imread(\"images/img_grid.png\"), cv2.COLOR_BGR2RGB))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T07:54:04.441084Z","iopub.status.idle":"2022-09-19T07:54:04.441839Z"},"trusted":true},"execution_count":null,"outputs":[]}]}