{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\n### In this notebook we use [SRGAN](https://arxiv.org/abs/1609.04802) to perform Super Resolution on [CelebA dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html).","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/1000/1*-txs0CYANMq5CVi0tp5DCg.png\" width=\"900\" height=\"900\"/>\n<h4></h4>\n<h4><center>Image Source:  <a href=\"https://arxiv.org/abs/1609.04802\">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network [C. Ledig et al.]</a></center></h4>","metadata":{}},{"cell_type":"markdown","source":"### Libraries üìö‚¨á","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, math, sys\nimport glob, itertools\nimport argparse, random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.models import vgg19\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image, make_grid\n\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-09-17T22:35:04.757358Z","iopub.execute_input":"2022-09-17T22:35:04.757665Z","iopub.status.idle":"2022-09-17T22:35:09.449776Z","shell.execute_reply.started":"2022-09-17T22:35:04.757626Z","shell.execute_reply":"2022-09-17T22:35:09.448361Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Settings ‚öôÔ∏è","metadata":{}},{"cell_type":"code","source":"# load pretrained models\nload_pretrained_models = True\n# number of epochs of training\nn_epochs = 2\n# name of the dataset\ndataset_path = \"../input/celeba-dataset/img_align_celeba/img_align_celeba\"\n#dataset_path = \"../input/div2k-valid/DIV2K_valid_HR\"\n# size of the batches\nbatch_size = 16\n# adam: learning rate\nlr = 0.0001\n# adam: decay of first order momentum of gradient\nb1 = 0.5\n# adam: decay of second order momentum of gradient\nb2 = 0.999\n# epoch from which to start lr decay\ndecay_epoch = 100\n# number of cpu threads to use during batch generation\nn_cpu = 8\n# high res. image height\nhr_height = 3600\n# high res. image width\nhr_width = 3600\n# number of image channels\nchannels = 3\n\nos.makedirs(\"images\", exist_ok=True)\nos.makedirs(\"saved_models\", exist_ok=True)\n\ncuda = torch.cuda.is_available()\nhr_shape = (hr_height, hr_width)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T22:35:09.454027Z","iopub.execute_input":"2022-09-17T22:35:09.454873Z","iopub.status.idle":"2022-09-17T22:35:09.568821Z","shell.execute_reply.started":"2022-09-17T22:35:09.454824Z","shell.execute_reply":"2022-09-17T22:35:09.567853Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Define Dataset Class","metadata":{}},{"cell_type":"code","source":"# Normalization parameters for pre-trained PyTorch models\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\nclass ImageDataset(Dataset):\n    def __init__(self, files, hr_shape):\n        hr_height, hr_width = hr_shape\n        # Transforms for low resolution images and high resolution images\n        self.lr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.hr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.files = files\n    \n    def __getitem__(self, index):\n        img = Image.open(self.files[index % len(self.files)])\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n\n        return {\"lr\": img_lr, \"hr\": img_hr}\n\n    def __len__(self):\n        return len(self.files)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T22:35:09.573654Z","iopub.execute_input":"2022-09-17T22:35:09.574183Z","iopub.status.idle":"2022-09-17T22:35:09.590742Z","shell.execute_reply.started":"2022-09-17T22:35:09.574150Z","shell.execute_reply":"2022-09-17T22:35:09.589226Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Get Train/Test Dataloaders","metadata":{}},{"cell_type":"code","source":"'''\ntrain_paths, test_paths = train_test_split(sorted(glob.glob(dataset_path + \"/*.*\")), test_size=0.02, random_state=42)\ntrain_dataloader = DataLoader(ImageDataset(train_paths, hr_shape=hr_shape), batch_size=batch_size, shuffle=True, num_workers=n_cpu)\ntest_dataloader = DataLoader(ImageDataset(test_paths, hr_shape=hr_shape), batch_size=int(batch_size*0.75), shuffle=True, num_workers=n_cpu)\n'''","metadata":{"execution":{"iopub.status.busy":"2022-09-17T22:35:09.593672Z","iopub.execute_input":"2022-09-17T22:35:09.594584Z","iopub.status.idle":"2022-09-17T22:35:09.612233Z","shell.execute_reply.started":"2022-09-17T22:35:09.594538Z","shell.execute_reply":"2022-09-17T22:35:09.610945Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'\\ntrain_paths, test_paths = train_test_split(sorted(glob.glob(dataset_path + \"/*.*\")), test_size=0.02, random_state=42)\\ntrain_dataloader = DataLoader(ImageDataset(train_paths, hr_shape=hr_shape), batch_size=batch_size, shuffle=True, num_workers=n_cpu)\\ntest_dataloader = DataLoader(ImageDataset(test_paths, hr_shape=hr_shape), batch_size=int(batch_size*0.75), shuffle=True, num_workers=n_cpu)\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"<h3><center>Model Architecture</center></h3>\n<img src=\"https://miro.medium.com/max/1000/1*zsiBj3IL4ALeLgsCeQ3lyA.png\" width=\"900\" height=\"900\"/>\n<h4></h4>\n<h4><center>Image Source:  <a href=\"https://arxiv.org/abs/1609.04802\">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network [C. Ledig et al.]</a></center></h4>","metadata":{}},{"cell_type":"markdown","source":"### Define Model Classes","metadata":{}},{"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n\n    def forward(self, img):\n        return self.feature_extractor(img)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n            nn.PReLU(),\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n        )\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n\n\nclass GeneratorResNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n        super(GeneratorResNet, self).__init__()\n\n        # First layer\n        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n\n        # Residual blocks\n        res_blocks = []\n        for _ in range(n_residual_blocks):\n            res_blocks.append(ResidualBlock(64))\n        self.res_blocks = nn.Sequential(*res_blocks)\n\n        # Second conv layer post residual blocks\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n\n        # Upsampling layers\n        upsampling = []\n        for out_features in range(2):\n            upsampling += [\n                # nn.Upsample(scale_factor=2),\n                nn.Conv2d(64, 256, 3, 1, 1),\n                nn.BatchNorm2d(256),\n                nn.PixelShuffle(upscale_factor=2),\n                nn.PReLU(),\n            ]\n        self.upsampling = nn.Sequential(*upsampling)\n\n        # Final output layer\n        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n\n    def forward(self, x):\n        out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T22:35:09.616547Z","iopub.execute_input":"2022-09-17T22:35:09.617285Z","iopub.status.idle":"2022-09-17T22:35:09.650189Z","shell.execute_reply.started":"2022-09-17T22:35:09.617241Z","shell.execute_reply":"2022-09-17T22:35:09.648717Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Train Super Resolution GAN (SRGAN)","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\n    \n# Initialize generator and discriminator\ngenerator = GeneratorResNet()\ndiscriminator = Discriminator(input_shape=(channels, *hr_shape))\nfeature_extractor = FeatureExtractor()\n\n# Set feature extractor to inference mode\nfeature_extractor.eval()\n\n# Losses\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_content = torch.nn.L1Loss()\n\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    feature_extractor = feature_extractor.cuda()\n    criterion_GAN = criterion_GAN.cuda()\n    criterion_content = criterion_content.cuda()\n\n# Load pretrained models\nif load_pretrained_models:\n    generator.load_state_dict(torch.load(\"../input/generatorpth/generator.pth\"))\n    discriminator.load_state_dict(torch.load(\"../input/discriminatorpth/discriminator.pth\"))\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.Tensor","metadata":{"execution":{"iopub.status.busy":"2022-09-17T22:35:09.652393Z","iopub.execute_input":"2022-09-17T22:35:09.653286Z","iopub.status.idle":"2022-09-17T22:35:35.450411Z","shell.execute_reply.started":"2022-09-17T22:35:09.653253Z","shell.execute_reply":"2022-09-17T22:35:35.449298Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a483b26b05dc4595a925c98bb9276a14"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\n\ntest_paths = sorted(glob.glob(\"../input/cow-original/*.*\"))   \ntest_paths = sorted(glob.glob(\"../input/cow-scrateched/*.*\"))   \ntest_paths = sorted(glob.glob(\"../input/casca-img2/*.*\"))   \n\nreal_test_dataloader = DataLoader(ImageDataset(test_paths, hr_shape=hr_shape), batch_size=1, shuffle=True, num_workers=n_cpu)\n\nexamples = enumerate(real_test_dataloader)\nbatch_idx, imgs = next(examples)\n\ngenerator.eval(); discriminator.eval()\n\nimgs_lr = Variable(imgs[\"lr\"].type(Tensor))\nimgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n  \ngen_hr = generator(imgs_lr)\n\nimgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\nimgs_hr = make_grid(imgs_hr, nrow=1, normalize=True)\ngen_hr = make_grid(gen_hr, nrow=1, normalize=True)\nimgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n#img_grid = torch.cat((imgs_hr, imgs_lr, gen_hr), -1)\n#save_image(img_grid, f\"images/1.png\", normalize=False)\nsave_image(imgs_hr, f\"images/imgs_hr.png\", normalize=False)\nsave_image(gen_hr, f\"images/gen_hr.png\", normalize=False)\nsave_image(imgs_lr, f\"images/imgs_lr.png\", normalize=False)\n\nplt.imshow(cv2.imread(\"images/gen_hr.png\"))","metadata":{"execution":{"iopub.status.busy":"2022-09-17T22:35:35.455922Z","iopub.execute_input":"2022-09-17T22:35:35.458459Z","iopub.status.idle":"2022-09-17T22:35:39.520997Z","shell.execute_reply.started":"2022-09-17T22:35:35.458415Z","shell.execute_reply":"2022-09-17T22:35:39.517918Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-32a2baf07e52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mimgs_hr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mgen_hr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mimgs_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-7fb1685a8dc5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-7fb1685a8dc5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 15.90 GiB total capacity; 15.07 GiB already allocated; 97.75 MiB free; 15.08 GiB reserved in total by PyTorch)"],"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 15.90 GiB total capacity; 15.07 GiB already allocated; 97.75 MiB free; 15.08 GiB reserved in total by PyTorch)","output_type":"error"}]},{"cell_type":"code","source":"train_gen_losses, train_disc_losses, train_counter = [], [], []\ntest_gen_losses, test_disc_losses = [], []\ntest_counter = [idx*len(train_dataloader.dataset) for idx in range(1, n_epochs+1)]\n\n'''\nfor epoch in range(n_epochs):\n\n    ### Training\n    gen_loss, disc_loss = 0, 0\n    tqdm_bar = tqdm(train_dataloader, desc=f'Training Epoch {epoch} ', total=int(len(train_dataloader)))\n    for batch_idx, imgs in enumerate(tqdm_bar):\n        generator.train(); discriminator.train()\n        # Configure model input\n        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        \n        ### Train Generator\n        optimizer_G.zero_grad()\n        # Generate a high resolution image from low resolution input\n        gen_hr = generator(imgs_lr)\n        # Adversarial loss\n        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n        # Content loss\n        gen_features = feature_extractor(gen_hr)\n        real_features = feature_extractor(imgs_hr)\n        loss_content = criterion_content(gen_features, real_features.detach())\n        # Total loss\n        loss_G = loss_content + 1e-3 * loss_GAN\n        loss_G.backward()\n        optimizer_G.step()\n\n        ### Train Discriminator\n        optimizer_D.zero_grad()\n        # Loss of real and fake images\n        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n        # Total loss\n        loss_D = (loss_real + loss_fake) / 2\n        loss_D.backward()\n        optimizer_D.step()\n\n        gen_loss += loss_G.item()\n        train_gen_losses.append(loss_G.item())\n        disc_loss += loss_D.item()\n        train_disc_losses.append(loss_D.item())\n        train_counter.append(batch_idx*batch_size + imgs_lr.size(0) + epoch*len(train_dataloader.dataset))\n        tqdm_bar.set_postfix(gen_loss=gen_loss/(batch_idx+1), disc_loss=disc_loss/(batch_idx+1))\n\n    # Testing\n    gen_loss, disc_loss = 0, 0\n    tqdm_bar = tqdm(test_dataloader, desc=f'Testing Epoch {epoch} ', total=int(len(test_dataloader)))\n    for batch_idx, imgs in enumerate(tqdm_bar):\n        generator.eval(); discriminator.eval()\n        # Configure model input\n        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        \n        ### Eval Generator\n        # Generate a high resolution image from low resolution input\n        gen_hr = generator(imgs_lr)\n        # Adversarial loss\n        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n        # Content loss\n        gen_features = feature_extractor(gen_hr)\n        real_features = feature_extractor(imgs_hr)\n        loss_content = criterion_content(gen_features, real_features.detach())\n        # Total loss\n        loss_G = loss_content + 1e-3 * loss_GAN\n\n        ### Eval Discriminator\n        # Loss of real and fake images\n        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n        # Total loss\n        loss_D = (loss_real + loss_fake) / 2\n\n        gen_loss += loss_G.item()\n        disc_loss += loss_D.item()\n        tqdm_bar.set_postfix(gen_loss=gen_loss/(batch_idx+1), disc_loss=disc_loss/(batch_idx+1))\n        \n        # Save image grid with upsampled inputs and SRGAN outputs\n        if random.uniform(0,1)<0.1:\n            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n            imgs_hr = make_grid(imgs_hr, nrow=1, normalize=True)\n            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n            img_grid = torch.cat((imgs_hr, imgs_lr, gen_hr), -1)\n            save_image(img_grid, f\"images/{batch_idx}.png\", normalize=False)\n\n    test_gen_losses.append(gen_loss/len(test_dataloader))\n    test_disc_losses.append(disc_loss/len(test_dataloader))\n    \n    # Save model checkpoints\n    \n    if np.argmin(test_gen_losses) == len(test_gen_losses)-1:\n        torch.save(generator.state_dict(), \"saved_models/generator.pth\")\n        torch.save(discriminator.state_dict(), \"saved_models/discriminator.pth\")\n    \n'''      ","metadata":{"execution":{"iopub.status.busy":"2022-09-17T22:35:39.523244Z","iopub.status.idle":"2022-09-17T22:35:39.524289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_gen_losses, mode='lines', name='Train Generator Loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_gen_losses, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Generator Loss'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Generator Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Adversarial + Content Loss\"),\nfig.show()\n'''","metadata":{"execution":{"iopub.status.busy":"2022-09-17T22:35:39.525926Z","iopub.status.idle":"2022-09-17T22:35:39.527043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_disc_losses, mode='lines', name='Train Discriminator Loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_disc_losses, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Discriminator Loss'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Discriminator Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Adversarial Loss\"),\nfig.show()\n'''","metadata":{"execution":{"iopub.status.busy":"2022-09-17T22:35:39.528621Z","iopub.status.idle":"2022-09-17T22:35:39.529712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"### Work in Progress ...\n### Note: The order of the below saved images are (from left-to-right): Original HR, Bicubic Interpolated & SRGAN Generated","metadata":{"execution":{"iopub.status.busy":"2022-09-17T22:35:39.531356Z","iopub.status.idle":"2022-09-17T22:35:39.532388Z"},"trusted":true},"execution_count":null,"outputs":[]}]}